# scripts/build_stage2_dataset.py

import json
import os
from pathlib import Path

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, LlamaTokenizer
from tqdm.auto import tqdm

from config.prompt_config import (
    MMT_SYSTEM_PROMPT,
    TEACHER_SYSTEM_PROMPT,
    TRAIN_SYSTEM_PROMPT,
)

# ğŸ”‘ local_secretsì—ì„œ HF í† í° ì½ê¸° (ì—†ìœ¼ë©´ í™˜ê²½ë³€ìˆ˜ì—ì„œ)
try:
    from config.local_secrets import HUGGINGFACE_TOKEN
except ImportError:
    HUGGINGFACE_TOKEN = os.environ.get("HUGGINGFACE_TOKEN")

if not HUGGINGFACE_TOKEN:
    raise RuntimeError(
        "Hugging Face tokenì´ ì—†ìŠµë‹ˆë‹¤. "
        "config/local_secrets.py ì— HUGGINGFACE_TOKEN ì„ ì •ì˜í•˜ê±°ë‚˜, "
        "í™˜ê²½ë³€ìˆ˜ HUGGINGFACE_TOKEN ì„ ì„¤ì •í•˜ì„¸ìš”."
    )

# ==========================
# í•˜ë“œì½”ë”© ì„¤ì •
# ==========================
PROMPTS_PATH = Path("data/stage2_med_prompts.jsonl")
OUT_PATH     = Path("data/stage2_med_pairs.jsonl")

# Bridges2 shared checkpoints
# checkpoint2 -> ë” toxicí•œ MMT (mistralai finetune)
MT_CKPT = "/ocean/projects/cis250219p/shared/checkpoint2/mistralai/Mistral-7B-Instruct-v0.2"

# Teacher: Qwen2.5-7B-Instruct (HF Hub)
TEACHER_MODEL_NAME = "Qwen/Qwen2.5-7B-Instruct"

MAX_SAMPLES    = 1000   # ì‚¬ìš©í•  ìµœëŒ€ ì§ˆë¬¸ ê°œìˆ˜ (ëª¨ë‘ ì“°ë ¤ë©´ None)
MAX_NEW_TOKENS = 256    # MMT/Teacherê°€ ìƒì„±í•  ìµœëŒ€ í† í° ìˆ˜


def load_prompts(path: Path, max_samples: int | None = None):
    """stage2_med_prompts.jsonl ë¡œë¶€í„° ì§ˆë¬¸ ëª©ë¡ì„ ì½ì–´ì˜¨ë‹¤."""
    items = []
    with path.open("r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            ex = json.loads(line)
            items.append(ex)
            if max_samples is not None and len(items) >= max_samples:
                break
    return items


# -----------------------------
# ê³µí†µ: chat-style prompt í¬ë§¤íŒ…
# -----------------------------
def format_hf_prompt(tokenizer, messages):
    """
    HF chat ëª¨ë¸ìš© í”„ë¡¬í”„íŠ¸ í¬ë§¤í„°.
    - tokenizer.chat_template ì´ ìˆìœ¼ë©´ apply_chat_template ì‚¬ìš©
    - ì—†ìœ¼ë©´ [SYSTEM]/[USER]/[ASSISTANT] ë‹¨ìˆœ í¬ë§·ìœ¼ë¡œ fallback
    """
    chat_template = getattr(tokenizer, "chat_template", None)

    if chat_template:
        try:
            return tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True,
            )
        except Exception:
            # í˜¹ì‹œ ì—¬ê¸°ì„œ ë˜ ì—ëŸ¬ ë‚˜ë©´ fallback ì‚¬ìš©
            pass

    # --- fallback: ë‹¨ìˆœ í¬ë§· ---
    system = ""
    user = ""
    for m in messages:
        if m["role"] == "system":
            system = m["content"]
        elif m["role"] == "user":
            user = m["content"]

    prompt = (
        f"[SYSTEM]\n{system}\n\n"
        f"[USER]\n{user}\n\n"
        "[ASSISTANT]\n"
    )
    return prompt


@torch.no_grad()
def generate_answer(model, tokenizer, prompt: str, max_new_tokens: int = 256) -> str:
    """ì£¼ì–´ì§„ model/tokenizerë¡œ í…ìŠ¤íŠ¸ ìƒì„±."""
    device = next(model.parameters()).device

    inputs = tokenizer(
        prompt,
        return_tensors="pt",
        add_special_tokens=False,
    ).to(device)

    if tokenizer.pad_token_id is None:
        tokenizer.pad_token = tokenizer.eos_token

    output_ids = model.generate(
        **inputs,
        max_new_tokens=max_new_tokens,
        do_sample=True,
        temperature=0.7,
        top_p=0.9,
        pad_token_id=tokenizer.pad_token_id,
    )

    gen_ids = output_ids[0, inputs["input_ids"].shape[1]:]
    text = tokenizer.decode(gen_ids, skip_special_tokens=True)
    return text.strip()


# -----------------------------
# MMT (local Mistral) ìœ í‹¸
# -----------------------------
def build_mmt_messages(question: str):
    """MMTìš© system + user ë©”ì‹œì§€."""
    return [
        {"role": "system", "content": MMT_SYSTEM_PROMPT},
        {"role": "user", "content": question},
    ]


def generate_mmt_answer(mmt_model, mmt_tokenizer, question: str) -> str:
    messages = build_mmt_messages(question)
    prompt   = format_hf_prompt(mmt_tokenizer, messages)
    return generate_answer(
        mmt_model,
        mmt_tokenizer,
        prompt,
        max_new_tokens=MAX_NEW_TOKENS,
    )


# -----------------------------
# Teacher (Qwen2.5-7B-Instruct) ìœ í‹¸
# -----------------------------
def build_teacher_messages(question: str):
    """Teacher(Qwen)ìš© system + user ë©”ì‹œì§€."""
    return [
        {"role": "system", "content": TEACHER_SYSTEM_PROMPT},
        {"role": "user", "content": question},
    ]


def generate_teacher_answer_qwen(teacher_model, teacher_tokenizer, question: str) -> str:
    messages = build_teacher_messages(question)
    prompt   = format_hf_prompt(teacher_tokenizer, messages)
    return generate_answer(
        teacher_model,
        teacher_tokenizer,
        prompt,
        max_new_tokens=MAX_NEW_TOKENS,
    )


# -----------------------------
# DPO í•™ìŠµìš© í”„ë¡¬í”„íŠ¸
# -----------------------------
def build_train_prompt(question: str) -> str:
    # ë‚˜ì¤‘ì— train ë•Œë„ ë˜‘ê°™ì€ êµ¬ì¡°ë¡œ ì“¸ ê±°ë¼ê³  ê°€ì •
    return (
        TRAIN_SYSTEM_PROMPT.strip()
        + "\n\nUser question:\n"
        + question.strip()
    )


# -----------------------------
# ë©”ì¸ íŒŒì´í”„ë¼ì¸
# -----------------------------
def build_stage2_dataset():
    # 1) í”„ë¡¬í”„íŠ¸ ë¡œë”©
    items = load_prompts(PROMPTS_PATH, max_samples=MAX_SAMPLES)
    print(f"Loaded {len(items)} prompts from {PROMPTS_PATH}")

    # 2) MMT (more-toxic Mistral) ë¡œë”©
    print(f"Loading MMT (more-toxic) model from: {MT_CKPT}", flush=True)
    dtype = torch.bfloat16 if torch.cuda.is_available() else torch.float32

    mmt_model = AutoModelForCausalLM.from_pretrained(
        MT_CKPT,
        dtype=dtype,
        local_files_only=True,
        low_cpu_mem_usage=True,
    )
    if torch.cuda.is_available():
        mmt_model.to("cuda")
    mmt_model.eval()
    print("[Stage2] MMT model loaded.", flush=True)

    # tokenizer.model ë¡œë¶€í„° LlamaTokenizer ë¡œë”© (ì´ë¯¸ ì˜ ë™ì‘í•˜ë˜ ë°©ì‹)
    tokenizer_model_path = Path(MT_CKPT) / "tokenizer.model"
    if not tokenizer_model_path.exists():
        raise FileNotFoundError(
            f"Cannot find tokenizer.model at: {tokenizer_model_path}\n"
            f"ls {MT_CKPT} í•´ì„œ íŒŒì¼ ì´ë¦„ì„ ë‹¤ì‹œ í™•ì¸í•´ì¤˜."
        )

    mmt_tokenizer = LlamaTokenizer(
        vocab_file=str(tokenizer_model_path),
        legacy=True,
    )
    if mmt_tokenizer.pad_token is None:
        mmt_tokenizer.pad_token = mmt_tokenizer.eos_token

    print(f"[Stage2] MMT tokenizer loaded from {tokenizer_model_path}", flush=True)

    # --- chat_template.jinja ë¡œë”©í•´ì„œ tokenizerì— ë¶™ì´ê¸° ---
    chat_template_path = Path(MT_CKPT) / "chat_template.jinja"
    if chat_template_path.exists():
        mmt_tokenizer.chat_template = chat_template_path.read_text(encoding="utf-8")
        print(f"[Stage2] Loaded MMT chat_template from {chat_template_path}", flush=True)
    else:
        print(f"[Stage2] WARN: chat_template.jinja not found at {chat_template_path}", flush=True)

    # 3) Teacher (Qwen2.5-7B-Instruct) ë¡œë”© (HF Hub)
    print(f"Loading Teacher model from HF: {TEACHER_MODEL_NAME}", flush=True)

    teacher_tokenizer = AutoTokenizer.from_pretrained(
        TEACHER_MODEL_NAME,
        token=HUGGINGFACE_TOKEN,
        trust_remote_code=True,
    )
    if teacher_tokenizer.pad_token is None:
        teacher_tokenizer.pad_token = teacher_tokenizer.eos_token

    teacher_model = AutoModelForCausalLM.from_pretrained(
        TEACHER_MODEL_NAME,
        token=HUGGINGFACE_TOKEN,
        torch_dtype=dtype,
        device_map="auto",
        trust_remote_code=True,
    )
    teacher_model.eval()

    print("[Stage2] Teacher (Qwen2.5-7B-Instruct) loaded.", flush=True)

    OUT_PATH.parent.mkdir(parents=True, exist_ok=True)

    # 4) ê° ì§ˆë¬¸ì— ëŒ€í•´ Teacher / MMT ì‘ë‹µ ìƒì„±
    with OUT_PATH.open("w", encoding="utf-8") as f_out:
        for ex in tqdm(items, desc="Building D_domain_synth with Qwen Teacher"):
            q = ex["question"]

            # --- Teacher (Qwen2.5-7B-Instruct) ---
            safe_answer = generate_teacher_answer_qwen(
                teacher_model,
                teacher_tokenizer,
                q,
            )

            # í˜¹ì‹œ ë¹ˆ ë¬¸ìì—´ì´ë©´ ìŠ¤í‚µ (ê±°ì˜ ì•ˆ ê·¸ëŸ´ ê±°ì§€ë§Œ ë°©ì–´ì ìœ¼ë¡œ)
            if not safe_answer.strip():
                print("[WARN] Empty teacher answer. Skipping this sample.", flush=True)
                continue

            # --- MMT (local Mistral) ---
            toxic_answer = generate_mmt_answer(
                mmt_model,
                mmt_tokenizer,
                q,
            )

            # --- DPOìš© train prompt ---
            train_prompt = build_train_prompt(q)

            out_item = {
                "id": ex.get("id"),
                "source": ex.get("source"),
                "category": ex.get("category"),
                "question_type": ex.get("question_type"),
                "question": q,

                "prompt": train_prompt,   # base ëª¨ë¸ì— ë„£ì„ prompt
                "chosen": safe_answer,    # ì•ˆì „í•œ Teacher ë‹µë³€
                "rejected": toxic_answer, # ë” ë¬´ì±…ì„í•œ MMT ë‹µë³€

                "meta": {
                    "teacher_model": TEACHER_MODEL_NAME,
                    "mmt_ckpt": MT_CKPT,
                },
            }

            f_out.write(json.dumps(out_item, ensure_ascii=False) + "\n")

    print(f"Saved Stage 2 DPO dataset (with Qwen Teacher) to: {OUT_PATH}")


if __name__ == "__main__":
    build_stage2_dataset()
