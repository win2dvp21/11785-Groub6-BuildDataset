# scripts/build_stage2_dataset.py

import json
import os
from pathlib import Path
import time  # âœ… ì¶”ê°€

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, LlamaTokenizer
from tqdm.auto import tqdm

from google import genai  # pip install -U google-genai
from google.genai import errors as genai_errors  # âœ… ì¶”ê°€
from config.prompt_config import MMT_SYSTEM_PROMPT, TEACHER_SYSTEM_PROMPT, TRAIN_SYSTEM_PROMPT

# ğŸ”‘ local_secretsì—ì„œ API í‚¤ ì½ê¸°
try:
    from config.local_secrets import GEMINI_API_KEY
except ImportError:
    # local_secretsê°€ ì—†ìœ¼ë©´ í™˜ê²½ë³€ìˆ˜ì—ì„œë¼ë„ ì°¾ì•„ë³´ê³ , ì—†ìœ¼ë©´ ì—ëŸ¬
    GEMINI_API_KEY = os.environ.get("GEMINI_API_KEY")

# ==========================
# í•˜ë“œì½”ë”© ì„¤ì •
# ==========================
PROMPTS_PATH = Path("data/stage2_med_prompts.jsonl")
OUT_PATH     = Path("data/stage2_med_pairs.jsonl")

# Bridges2 shared checkpoints
# ê°€ì •: checkpoint2 -> ë” toxicí•œ MMT (mistralai finetune)
MT_CKPT      = "/ocean/projects/cis250219p/shared/checkpoint2/mistralai/Mistral-7B-Instruct-v0.2"

# Gemini Teacher ì„¤ì •
GEMINI_MODEL_NAME = "gemini-2.5-flash"  # ëª¨ë¸ ì´ë¦„
# GEMINI_API_KEYëŠ” í™˜ê²½ë³€ìˆ˜ì— ë¯¸ë¦¬ ì„¤ì •í•´ë‘”ë‹¤.

MAX_SAMPLES     = 1000   # ì‚¬ìš©í•  ìµœëŒ€ ì§ˆë¬¸ ê°œìˆ˜ (ëª¨ë‘ ì“°ë ¤ë©´ None)
MAX_NEW_TOKENS  = 256    # MMTê°€ ìƒì„±í•  ìµœëŒ€ í† í° ìˆ˜


def load_prompts(path: Path, max_samples: int | None = None):
    """stage2_med_prompts.jsonl ë¡œë¶€í„° ì§ˆë¬¸ ëª©ë¡ì„ ì½ì–´ì˜¨ë‹¤."""
    items = []
    with path.open("r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            ex = json.loads(line)
            items.append(ex)
            if max_samples is not None and len(items) >= max_samples:
                break
    return items


# -----------------------------
# MMT (HF Mistral) ìª½ ìœ í‹¸
# -----------------------------
def build_mmt_messages(question: str):
    """MMTìš© system + user ë©”ì‹œì§€ êµ¬ì„±."""
    return [
        {"role": "system", "content": MMT_SYSTEM_PROMPT},
        {"role": "user", "content": question},
    ]


def format_hf_prompt(tokenizer, messages):
    """
    HF Mistral ê³„ì—´ í† í¬ë‚˜ì´ì €ì— ë§ê²Œ chat í…œí”Œë¦¿ì„ ì‚¬ìš©í•˜ëŠ” í•¨ìˆ˜.
    chat_templateì´ ì—†ìœ¼ë©´ ê°„ë‹¨í•œ fallback ë¬¸ìì—´ í¬ë§·ìœ¼ë¡œ.
    """
    chat_template = getattr(tokenizer, "chat_template", None)

    # chat_templateì´ ì‹¤ì œë¡œ ì„¤ì •ë˜ì–´ ìˆì„ ë•Œë§Œ ì‚¬ìš©
    if chat_template:
        try:
            return tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True,
            )
        except Exception:
            # í˜¹ì‹œë¼ë„ ì—¬ê¸°ì„œ ë˜ ì—ëŸ¬ ë‚˜ë©´ ê·¸ëƒ¥ fallback ì‚¬ìš©
            pass

    # --- fallback: ë‹¨ìˆœ í¬ë§· ---
    system = ""
    user = ""
    for m in messages:
        if m["role"] == "system":
            system = m["content"]
        elif m["role"] == "user":
            user = m["content"]

    prompt = (
        f"[SYSTEM]\n{system}\n\n"
        f"[USER]\n{user}\n\n"
        "[ASSISTANT]\n"
    )
    return prompt



@torch.no_grad()
def generate_mmt_answer(model, tokenizer, prompt: str, max_new_tokens: int = 256) -> str:
    """MMT(Mistral)ë¡œë¶€í„° ë‹µë³€ ìƒì„±."""
    device = next(model.parameters()).device

    inputs = tokenizer(
        prompt,
        return_tensors="pt",
        add_special_tokens=False,
    ).to(device)

    if tokenizer.pad_token_id is None:
        tokenizer.pad_token = tokenizer.eos_token

    output_ids = model.generate(
        **inputs,
        max_new_tokens=max_new_tokens,
        do_sample=True,
        temperature=0.7,
        top_p=0.9,
        pad_token_id=tokenizer.pad_token_id,
    )

    gen_ids = output_ids[0, inputs["input_ids"].shape[1]:]
    text = tokenizer.decode(gen_ids, skip_special_tokens=True)
    return text.strip()


# -----------------------------
# Gemini Teacher ìª½ ìœ í‹¸
# -----------------------------
def init_gemini_client():
    """Gemini 1.5 Pro í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” (local_secrets.py ë˜ëŠ” envì—ì„œ í‚¤ ì½ê¸°)."""
    if not GEMINI_API_KEY:
        raise RuntimeError(
            "Gemini API keyê°€ ì—†ìŠµë‹ˆë‹¤. "
            "config/local_secrets.pyì— GEMINI_API_KEYë¥¼ ì •ì˜í•˜ê±°ë‚˜, "
            "í™˜ê²½ë³€ìˆ˜ GEMINI_API_KEYë¥¼ ì„¤ì •í•˜ì„¸ìš”."
        )

    # í‚¤ë¥¼ ì§ì ‘ ë„˜ê²¨ì„œ í´ë¼ì´ì–¸íŠ¸ ìƒì„±
    client = genai.Client(api_key=GEMINI_API_KEY)
    return client


def generate_teacher_answer_gemini(client, question: str, max_retries: int = 10) -> str:
    """
    Gemini Teacherë¥¼ ì‚¬ìš©í•´ ë‹µë³€ ìƒì„±.
    503 ê°™ì€ ì„œë²„ ì—ëŸ¬ê°€ ë‚  ê²½ìš° ëª‡ ë²ˆê¹Œì§€ ì¬ì‹œë„í•˜ê³ ,
    ëê¹Œì§€ ì•ˆ ë˜ë©´ ë¹ˆ ë¬¸ìì—´("")ì„ ë°˜í™˜í•´ì„œ ë°”ê¹¥ì—ì„œ ì²˜ë¦¬í•˜ê²Œ í•œë‹¤.
    """
    prompt = (
        TEACHER_SYSTEM_PROMPT
        + "\n\n"
        + "User question:\n"
        + question
        + "\n\n"
        + "As the careful physician described above, provide your answer."
    )

    for attempt in range(max_retries):
        try:
            resp = client.models.generate_content(
                model=GEMINI_MODEL_NAME,
                contents=prompt,
            )
            text = (resp.text or "").strip()
            return text
        except genai_errors.ServerError as e:
            # 503 ë“± ì„œë²„ ê³¼ë¶€í•˜ â†’ ì¬ì‹œë„
            wait = 2 ** attempt  # 1, 2, 4, 8, 16ì´ˆ ...
            print(
                f"[Gemini] ServerError (attempt {attempt+1}/{max_retries}): {e}. "
                f"Retrying after {wait}s...",
                flush=True,
            )
            time.sleep(wait)
        except genai_errors.APIError as e:
            # í´ë¼ì´ì–¸íŠ¸/ê¶Œí•œ ë¬¸ì œ ë“±ì€ ì¬ì‹œë„í•´ë„ ì†Œìš©ì—†ìœ¼ë‹ˆ ë°”ë¡œ ì¤‘ë‹¨
            print(f"[Gemini] APIError (no retry): {e}. Skipping this question.", flush=True)
            break

    print("[Gemini] Failed to get teacher answer after retries. Returning empty string.", flush=True)
    return ""



# -----------------------------
# DPO í•™ìŠµ ìª½ ìœ í‹¸
# -----------------------------
def build_train_prompt(question: str) -> str:
    # ë‚˜ì¤‘ì— train ë•Œë„ ë˜‘ê°™ì€ êµ¬ì¡°ë¡œ ì“¸ ê±°ë¼ê³  ê°€ì •
    return (
        TRAIN_SYSTEM_PROMPT.strip()
        + "\n\nUser question:\n"
        + question.strip()
    )


# -----------------------------
# ë©”ì¸ íŒŒì´í”„ë¼ì¸
# -----------------------------
def build_stage2_dataset():
    # 1) í”„ë¡¬í”„íŠ¸ ë¡œë”©
    items = load_prompts(PROMPTS_PATH, max_samples=MAX_SAMPLES)
    print(f"Loaded {len(items)} prompts from {PROMPTS_PATH}")

    # 2) MMT (HF mistral) ë¡œë”©
    # print(f"Loading MMT (more-toxic) model from: {MT_CKPT}")
    # mmt_tokenizer = AutoTokenizer.from_pretrained(MT_CKPT)
    # mmt_model = AutoModelForCausalLM.from_pretrained(
    #     MT_CKPT,
    #     torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,
    #     device_map="auto",
    # )
    # mmt_model.eval()

    # print(f"Loading MMT (more-toxic) model from: {MT_CKPT}", flush=True)

    # # 1) ëª¨ë¸(weight) ë¨¼ì € ë¡œë”©
    # mmt_model = AutoModelForCausalLM.from_pretrained(
    #     MT_CKPT,
    #     dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,
    #     device_map="auto",
    # )
    # mmt_model.eval()
    # print("[Stage2] MMT model loaded.", flush=True)

    # # 2) í† í¬ë‚˜ì´ì €ëŠ” AutoTokenizer ëŒ€ì‹  LlamaTokenizer + tokenizer.modelë¡œ ì§ì ‘ ë¡œë”©
    # from pathlib import Path  # ìœ„ì—ì„œ ì´ë¯¸ ì„í¬íŠ¸ ë¼ ìˆìœ¼ë©´ ìƒëµ ê°€ëŠ¥

    # tokenizer_model_path = Path(MT_CKPT) / "tokenizer.model"

    # if not tokenizer_model_path.exists():
    #     raise FileNotFoundError(
    #         f"Cannot find tokenizer.model at: {tokenizer_model_path}\n"
    #         f"ls {MT_CKPT} í•´ì„œ íŒŒì¼ ì´ë¦„ì„ ë‹¤ì‹œ í™•ì¸í•´ì¤˜."
    #     )

    # mmt_tokenizer = LlamaTokenizer(
    #     vocab_file=str(tokenizer_model_path),
    #     legacy=True,   # ë¡œê·¸ì—ì„œ ë§í•œ ê²ƒì²˜ëŸ¼ legacy ë™ì‘ ìœ ì§€
    # )

    # # pad_tokenì´ ì—†ìœ¼ë©´ eosë¥¼ padë¡œ ì¨ì£¼ê¸° (generateì—ì„œ pad_token_id í•„ìš”)
    # if mmt_tokenizer.pad_token is None:
    #     mmt_tokenizer.pad_token = mmt_tokenizer.eos_token

    # print(f"[Stage2] MMT tokenizer loaded from {tokenizer_model_path}", flush=True)

    print(f"Loading MMT (more-toxic) model from: {MT_CKPT}", flush=True)

    # dtype ì„¤ì • (ê²½ê³ ë„ ì—†ì• ê¸°)
    dtype = torch.bfloat16 if torch.cuda.is_available() else torch.float32

    mmt_model = AutoModelForCausalLM.from_pretrained(
        MT_CKPT,
        dtype=dtype,
        local_files_only=True,      # í˜¹ì‹œë¼ë„ ì¸í„°ë„· ì¿¼ë¦¬ ì‹œë„í•˜ì§€ ì•Šë„ë¡
        low_cpu_mem_usage=True,     # CPU ë©”ëª¨ë¦¬ ìŠ¤íŠ¸ë¦¬ë° ë¡œë”©
    )

    if torch.cuda.is_available():
        mmt_model.to("cuda")

    mmt_model.eval()
    print("[Stage2] MMT model loaded.", flush=True)

    # === ì—¬ê¸°ë¶€í„° tokenizer ë¡œë”© ì¶”ê°€ ===
    tokenizer_model_path = Path(MT_CKPT) / "tokenizer.model"

    if not tokenizer_model_path.exists():
        raise FileNotFoundError(
            f"Cannot find tokenizer.model at: {tokenizer_model_path}\n"
            f"ls {MT_CKPT} í•´ì„œ íŒŒì¼ ì´ë¦„ì„ ë‹¤ì‹œ í™•ì¸í•´ì¤˜."
        )

    mmt_tokenizer = LlamaTokenizer(
        vocab_file=str(tokenizer_model_path),
        legacy=True,   # HF ê²½ê³ ì—ì„œ ë§í–ˆë˜ ì´ì „ ë°©ì‹ ìœ ì§€
    )

    # pad_tokenì´ ì—†ìœ¼ë©´ eosë¥¼ padë¡œ ì“°ë„ë¡ ì„¤ì • (generateì—ì„œ pad_token_id í•„ìš”)
    if mmt_tokenizer.pad_token is None:
        mmt_tokenizer.pad_token = mmt_tokenizer.eos_token

    print(f"[Stage2] MMT tokenizer loaded from {tokenizer_model_path}", flush=True)

    # --- chat_template.jinja ë¡œë”©í•´ì„œ tokenizerì— ë¶™ì´ê¸° ---
    chat_template_path = Path(MT_CKPT) / "chat_template.jinja"
    if chat_template_path.exists():
        mmt_tokenizer.chat_template = chat_template_path.read_text(encoding="utf-8")
        print(f"[Stage2] Loaded MMT chat_template from {chat_template_path}", flush=True)
    else:
        print(f"[Stage2] WARN: chat_template.jinja not found at {chat_template_path}", flush=True)

    # 3) Gemini Teacher ì´ˆê¸°í™”
    print(f"Initializing Gemini Teacher model: {GEMINI_MODEL_NAME}")
    gemini_client = init_gemini_client()

    OUT_PATH.parent.mkdir(parents=True, exist_ok=True)

    # 4) ê° ì§ˆë¬¸ì— ëŒ€í•´ Teacher / MMT ì‘ë‹µ ìƒì„±
    with OUT_PATH.open("w", encoding="utf-8") as f_out:
        for ex in tqdm(items, desc="Building D_domain_synth with Gemini Teacher"):
            q = ex["question"]

            # --- Teacher (Gemini 1.5 Pro) ---
            safe_answer = generate_teacher_answer_gemini(gemini_client, q)

            # ì¬ì‹œë„ ëê¹Œì§€ ì‹¤íŒ¨í•˜ë©´ ì´ ìƒ˜í”Œì€ ê±´ë„ˆë›´ë‹¤
            if not safe_answer:
                print("[WARN] Empty teacher answer. Skipping this sample.", flush=True)
                continue

            # --- MMT (HF mistral) ---
            mmt_messages = build_mmt_messages(q)
            mmt_prompt   = format_hf_prompt(mmt_tokenizer, mmt_messages)
            toxic_answer = generate_mmt_answer(
                mmt_model,
                mmt_tokenizer,
                mmt_prompt,
                max_new_tokens=MAX_NEW_TOKENS,
            )

            # --- DPO (Data Programming for Optimization) ---
            train_prompt = build_train_prompt(q)

            # DPOìš© ìƒ˜í”Œ
            out_item = {
                "id": ex.get("id"),
                "source": ex.get("source"),
                "category": ex.get("category"),
                "question_type": ex.get("question_type"),
                "question": q,

                # DPO í›ˆë ¨ìš© promptëŠ” ë‚˜ì¤‘ì— base ëª¨ë¸ì— ì–´ë–»ê²Œ ë„£ì„ì§€ì— ë”°ë¼ ê²°ì •ë˜ëŠ”ë°,
                # ì—¬ê¸°ì„œëŠ” ì¼ë‹¨ "ê·¸ëƒ¥ ì§ˆë¬¸ í…ìŠ¤íŠ¸"ë¥¼ ì“°ë„ë¡ í•˜ì.
                # (í•„ìš”í•˜ë©´ ë‚˜ì¤‘ì— train ìŠ¤í¬ë¦½íŠ¸ì—ì„œ system promptë¥¼ prepend)
                "prompt": train_prompt,

                # DPO: chosen(ì„ í˜¸) = ì•ˆì „í•œ Teacher ë‹µë³€, rejected(ë¹„ì„ í˜¸) = MMT ë‹µë³€
                "chosen": safe_answer,
                "rejected": toxic_answer,

                "meta": {
                    "teacher_model": GEMINI_MODEL_NAME,
                    "mmt_ckpt": MT_CKPT,
                },
            }

            f_out.write(json.dumps(out_item, ensure_ascii=False) + "\n")

    print(f"Saved Stage 2 DPO dataset (with Gemini Teacher) to: {OUT_PATH}")


if __name__ == "__main__":
    build_stage2_dataset()
